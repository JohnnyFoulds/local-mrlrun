{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 : GPU Check\n",
    "\n",
    "This is a simple test to see if the GPU is available and working correctly.\n",
    "\n",
    "- https://stackoverflow.com/questions/76581229/is-it-possible-to-check-if-gpu-is-available-without-using-deep-learning-packages\n",
    "- https://docs.mlrun.org/en/v1.7.2/runtimes/configuring-job-resources.html\n",
    "- https://docs.k3s.io/advanced#nvidia-container-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTTPRunDB('http://dragon:30070')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the API server URL\n",
    "mlrun.get_run_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-15 14:41:25,383 [info] Loading project from path: {\"path\":\"./\",\"project_name\":\"mlrun-demo\",\"user_project\":true}\n",
      "> 2025-07-15 14:41:25,452 [info] Project loaded successfully: {\"path\":\"./\",\"project_name\":\"mlrun-demo-johannes\",\"stored_in_db\":true}\n",
      "Full project name: mlrun-demo-johannes\n"
     ]
    }
   ],
   "source": [
    "# Set the base project name\n",
    "project_name = \"mlrun-demo\"\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name, \n",
    "    context=\"./\",\n",
    "    user_project=True)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GPU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 02_get_gpu_info.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 02_get_gpu_info.py\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "def get_gpu_info(context):    \n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_info = []\n",
    "    for gpu in gpus:\n",
    "        gpu_info.append({\n",
    "            'id': gpu.id,\n",
    "            'name': gpu.name,\n",
    "            'load': gpu.load,\n",
    "            'memory_total': gpu.memoryTotal,\n",
    "            'memory_free': gpu.memoryFree,\n",
    "            'memory_used': gpu.memoryUsed,\n",
    "        })\n",
    "\n",
    "    context.logger.info(f\"GPU Info: {gpu_info}\")\n",
    "    return gpu_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Run Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_gpu_check = project.set_function(\n",
    "    func=\"02_get_gpu_info.py\",\n",
    "    name=\"gpu-check\",\n",
    "    tag=\"latest\",\n",
    "    kind=\"job\",\n",
    "    image=\"mlrun/mlrun-gpu\",\n",
    "    handler=\"get_gpu_info\",\n",
    "    requirements=[\"GPUtil==1.4.0\"])\n",
    "\n",
    "# Then set the GPU resources on the function's spec\n",
    "fn_gpu_check.with_requests(mem=\"1G\", cpu=1)  # lower bound\n",
    "fn_gpu_check.with_limits(mem=\"2G\", cpu=2, gpus=1)  # upper bound\n",
    "# fn_gpu_check.spec.resources = {\n",
    "#     \"limits\": {\"nvidia.com/gpu\": 1},\n",
    "#     \"requests\": {\"nvidia.com/gpu\": 1}\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-15 14:41:25,536 [warning] it is recommended to use k8s secret (specify secret_name), specifying the aws_access_key/aws_secret_key directly is unsafe\n",
      "> 2025-07-15 14:41:25,553 [error] error getting build status: details: MLRunNotFoundError('Function tag not found mlrun-demo-johannes/gpu-check:latest'), caused by: 404 Client Error: Not Found for url: http://dragon:30070/api/v1/build/status?name=gpu-check&project=mlrun-demo-johannes&tag=latest&logs=no&offset=0&last_log_timestamp=0.0&verbose=no\n",
      "> 2025-07-15 14:41:25,553 [info] Function is not deployed and auto_build flag is set, starting deploy...\n",
      "> 2025-07-15 14:41:25,561 [error] error getting build status: details: MLRunNotFoundError('Function tag not found mlrun-demo-johannes/gpu-check:latest'), caused by: 404 Client Error: Not Found for url: http://dragon:30070/api/v1/build/status?name=gpu-check&project=mlrun-demo-johannes&tag=latest&logs=no&offset=0&last_log_timestamp=0.0&verbose=no\n",
      "> 2025-07-15 14:41:25,562 [warning] Even though skip_deployed=True, the build might be triggered due to the function's configuration. See requires_build() and is_deployed() for reasoning.\n",
      "> 2025-07-15 14:41:25,621 [info] Started building image: .shambi/func-mlrun-demo-johannes-gpu-check:latest\n"
     ]
    }
   ],
   "source": [
    "# run the function locally\n",
    "fn_gpu_check.run(\n",
    "    local=False,\n",
    "    handler=\"get_gpu_info\",\n",
    "    auto_build=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
