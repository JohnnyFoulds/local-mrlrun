# References

## Hugging Face

- [Cache management](https://huggingface.co/docs/datasets/en/cache)
- [Download an entire repository](https://huggingface.co/docs/huggingface_hub/en/guides/download#download-an-entire-repository)

## MLRun

- [MLRun Quick Start Tutorial: YouTube](https://www.youtube.com/watch?v=xI8KVGLlj7Q)
- [Kubeflow Vs. MLflow Vs. MLRun: Which One is Right for You?](https://www.iguazio.com/blog/kubeflow-vs-mlflow-vs-mlrun/)
- [MLRun Tutorial: How to Train, Compare, and Register Models: YouTube](https://www.youtube.com/watch?v=bZgBsmLMdQo)
- [Quick start tutorial for machine learning](https://docs.mlrun.org/en/latest/tutorials/01-mlrun-basics.html)
- [Train, compare, and register models](https://docs.mlrun.org/en/latest/tutorials/02-model-training.html)
- [Create, save, and use projects](https://docs.mlrun.org/en/stable/projects/create-project.html)
- [Serving gen AI models](https://docs.mlrun.org/en/v1.9.1/genai/deployment/genai_serving.html)

### Setup

- [MLRun Setup](https://github.com/mlrun/mlrun-setup/)

## OpenShift

- [How to Set Up OpenShift Local (CRC) on Ubuntu: A Developer's Guide](https://dev.to/khurammurad/how-to-set-up-openshift-local-crc-on-ubuntu-a-developers-guide-le2)

## S3

- [Garage: Deploying on Kubernetes](https://garagehq.deuxfleurs.fr/documentation/cookbook/kubernetes/)

## PyPI

- [Hosting your own simple repository](https://packaging.python.org/en/latest/guides/hosting-your-own-index/)

## vLLM

- [Offline Inference: Qwen2.5-Omni](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/qwen2_5_omni/only_thinker.py)
- [Loading models with Run:ai Model Streamer](https://docs.vllm.ai/en/stable/models/extensions/runai_model_streamer.html)
- [Stream Model From AWS S3](https://github.com/run-ai/runai-model-streamer/blob/master/examples/stream_safetensors_from_s3.ipynb)
- [vLLM Recipes](https://docs.vllm.ai/projects/recipes/en/latest/)
- [Loading models with CoreWeave's Tensorizer](https://github.com/vllm-project/vllm/blob/458e74eb907f96069e6d8a4f3c9f457001fef2ea/docs/models/extensions/tensorizer.md)
- [AutoAWQ](https://docs.vllm.ai/en/v0.9.0/features/quantization/auto_awq.html)

## LLM Models

## DeepSeek R1

- [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)
- [casperhansen/deepseek-r1-distill-qwen-14b-awq](https://huggingface.co/casperhansen/deepseek-r1-distill-qwen-14b-awq/tree/main)
- [vLLM: DeepSeek-V3 (R1) Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3.html)
- [DeepSeek R1 + Aider + Cline3.2 + VLLM: SOTA Free AI Coder on Multi-GPUs with Distributed Inferencing](https://www.youtube.com/watch?v=yKiga4WHRTc)
- [DeepSeek R1 + VLLM + Cline 3.2: Run Open Stack AI Coder on Multi-GPUs with Distributed Inferencing](https://www.youtube.com/watch?v=Quwf1TBycgM)
- [DeepSeek-R1-Distill-Qwen-32B + VLLM + OpenWebUI + SearXNG: Best Local Free Replica for DeepSeek Chat](https://www.youtube.com/watch?v=mFDEUvnPdFg)

## GPT-OSS

- [GPT-OSS: Hype](https://www.youtube.com/watch?v=NyW7EDFmWl4)
- [Let's Run OpenAI GPT-OSS - Official Open Source ChatGPT | Developer Review](https://www.youtube.com/watch?v=mlpFG8e_fLw)
- [vLLM: gpt-oss Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html)