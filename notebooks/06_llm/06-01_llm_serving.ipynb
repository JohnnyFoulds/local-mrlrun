{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06-01 : LLM Serving\n",
    "\n",
    "A basic test to serve a LLM as a function.\n",
    "\n",
    "## Refrences\n",
    "\n",
    "- [Deploying an LLM using MLRun](https://docs.mlrun.org/en/v1.7.2/tutorials/genai_01_basic_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the API server URL\n",
    "mlrun.get_run_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"microsoft/phi-2\" # the model ID to use\n",
    "project_name = \"llm-serving\" # the project name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name,\n",
    "    user_project=True)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model Cache Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cache directory for the model\n",
    "CACHE_DIR = mlrun.mlconf.artifact_path\n",
    "CACHE_DIR = (\n",
    "    CACHE_DIR.replace(\"v3io://\", \"/v3io\").replace(\"{{run.project}}\", project.name)\n",
    "    + \"/cache\"\n",
    ")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements for the function\n",
    "requirements = [\n",
    "    \"transformers==4.41.2\",\n",
    "    \"tensorflow==2.16.1\",\n",
    "    \"torch\"\n",
    "]\n",
    "\n",
    "# create the function to serve the model \n",
    "serve_func = project.set_function(\n",
    "    name=\"serve-llm\",\n",
    "    func=\"src/06-01_serving.py\",\n",
    "    image=\"dragon:6500/mlrun/mlrun-gpu:1.7.2\",\n",
    "    kind=\"nuclio\",\n",
    "    handler=\"invoke_llm\",\n",
    "    requirements=requirements\n",
    ").apply(mlrun.auto_mount())\n",
    "\n",
    "# set the environment variables for the function\n",
    "serve_func.set_envs(env_vars={\n",
    "    \"MODEL_ID\": MODEL_ID, \n",
    "    \"CACHE_DIR\": CACHE_DIR\n",
    "})\n",
    "\n",
    "# Since the model is stored in memory, use only 1 replica and and one worker\n",
    "# Since this is running on CPU only, inference might take ~1 minute (increasing timeout)\n",
    "serve_func.spec.min_replicas = 1\n",
    "serve_func.spec.max_replicas = 1\n",
    "serve_func.with_http(worker_timeout=120, gateway_timeout=150, workers=1)\n",
    "serve_func.set_config(\"spec.readinessTimeoutSeconds\", 1200)\n",
    "\n",
    "# set gpu resources for the function\n",
    "serve_func.with_limits(gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the function\n",
    "serve_func = project.deploy_function(function=\"serve-llm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
