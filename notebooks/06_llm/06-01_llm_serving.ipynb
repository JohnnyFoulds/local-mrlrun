{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06-01 : LLM Serving\n",
    "\n",
    "A basic test to serve a LLM as a function.\n",
    "\n",
    "> ❗❗️This was more an infrastructure test to make sure nuclio can build the image and push it to the local registry. It is also useful to monitor the network traffic with `iftop` to see the impacts of downloding container images, package libraries, and finally the llm; to see where the bottlenecks are. Also watch disk usage with `watch -t 'df | head'`.\n",
    "\n",
    "## Refrences\n",
    "\n",
    "- [Deploying an LLM using MLRun](https://docs.mlrun.org/en/v1.7.2/tutorials/genai_01_basic_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTTPRunDB('http://dragon.local:30070')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the API server URL\n",
    "mlrun.get_run_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"microsoft/phi-2\" # the model ID to use\n",
    "project_name = \"llm-serving\" # the project name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-31 13:35:55,478 [info] Project loaded successfully: {\"project_name\":\"llm-serving-johannes\"}\n",
      "Full project name: llm-serving-johannes\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name,\n",
    "    user_project=True)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model Cache Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: s3://mlrun/projects/llm-serving-johannes/artifacts/cache\n"
     ]
    }
   ],
   "source": [
    "# the cache directory for the model\n",
    "CACHE_DIR = mlrun.mlconf.artifact_path\n",
    "CACHE_DIR = (\n",
    "    CACHE_DIR.replace(\"v3io://\", \"/v3io\").replace(\"{{run.project}}\", project.name)\n",
    "    + \"/cache\"\n",
    ")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-31 13:35:55,540 [warning] Adding HTTP trigger despite the default HTTP trigger creation being disabled\n"
     ]
    }
   ],
   "source": [
    "# requirements for the function\n",
    "requirements = [\n",
    "    \"transformers==4.41.2\",\n",
    "    \"tensorflow==2.16.1\",\n",
    "    \"torch\"\n",
    "]\n",
    "\n",
    "# create the function to serve the model \n",
    "image = \"registry-service.mlrun.svc.cluster.local/mlrun/mlrun-gpu:1.9.1-py39\"  # specify the image to use\n",
    "serve_func = project.set_function(\n",
    "    name=\"serve-llm\",\n",
    "    func=\"src/06-01_serving.py\",\n",
    "    image=image,\n",
    "    kind=\"nuclio\",\n",
    "    handler=\"invoke_llm\",\n",
    "    requirements=requirements\n",
    ")\n",
    "\n",
    "# set the environment variables for the function\n",
    "serve_func.set_envs(env_vars={\n",
    "    \"MODEL_ID\": MODEL_ID, \n",
    "    \"CACHE_DIR\": CACHE_DIR\n",
    "})\n",
    "\n",
    "# Since the model is stored in memory, use only 1 replica and and one worker\n",
    "# Since this is running on CPU only, inference might take ~1 minute (increasing timeout)\n",
    "serve_func.spec.min_replicas = 1\n",
    "serve_func.spec.max_replicas = 1\n",
    "serve_func.with_http(worker_timeout=120, gateway_timeout=150, workers=1)\n",
    "serve_func.set_config(\"spec.readinessTimeoutSeconds\", 1200)\n",
    "\n",
    "# set gpu resources for the function\n",
    "serve_func.with_limits(gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-31 13:35:55,546 [warning] It is recommended to use k8s secret (specify secret_name), specifying aws_access_key/aws_secret_key directly is unsafe.\n",
      "> 2025-07-31 13:35:55,548 [info] Starting remote function deploy\n",
      "2025-07-31 11:35:55  (info) Deploying function\n",
      "2025-07-31 11:35:55  (info) Building\n",
      "2025-07-31 11:35:55  (info) Staging files and preparing base images\n",
      "2025-07-31 11:35:55  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2025-07-31 11:35:55  (info) Building processor image\n",
      "2025-07-31 11:39:42  (info) Build complete\n",
      "Failed to deploy. Details:\n",
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 11.8.0\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n",
      "   Use the NVIDIA Container Toolkit to start this container with GPU support; see\n",
      "   https://docs.nvidia.com/datacenter/cloud-native/ .\n",
      "\n",
      "25.07.31 11:46:20.427 \u001b[34m(I)\u001b[0m \u001b[37m                processor\u001b[0m Starting processor {\"version\": \"Label: 1.13.24, Git commit: e20b1b793e4e9f4444e2d2b25edcfd3fa9053ae4, OS: linux, Arch: amd64, Go version: go1.23.8\"}\n",
      "25.07.31 11:46:20.427 \u001b[32m(D)\u001b[0m \u001b[37m                processor\u001b[0m Read configuration {\"config\": \"{\n",
      "    \"metadata\": {\n",
      "        \"name\": \"llm-serving-johannes-serve-llm\",\n",
      "        \"namespace\": \"mlrun\",\n",
      "        \"labels\": {\n",
      "            \"mlrun/class\": \"remote\",\n",
      "            \"nuclio.io/app\": \"functionres\",\n",
      "            \"nuclio.io/class\": \"function\",\n",
      "            \"nuclio.io/function-name\": \"llm-serving-johannes-serve-llm\",\n",
      "            \"nuclio.io/function-version\": \"latest\",\n",
      "            \"nuclio.io/project-name\": \"llm-serving-johannes\",\n",
      "            \"nuclio.io/tag\": \"latest\"\n",
      "        },\n",
      "        \"annotations\": {\n",
      "            \"nuclio.io/generated_by\": \"function generated from git@github.com:JohnnyFoulds/local-mrlrun.git#f3ee8964cbd8a9a4f49df41d2153aefc6a96a775:src/06-01_serving.py\"\n",
      "        }\n",
      "    },\n",
      "    \"spec\": {\n",
      "        \"handler\": \"06-01-serving-nuclio:invoke_llm\",\n",
      "        \"runtime\": \"python:3.9\",\n",
      "        \"env\": [\n",
      "            {\n",
      "                \"name\": \"MLRUN_HTTPDB__NUCLIO__EXPLICIT_ACK\",\n",
      "                \"value\": \"enabled\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"MODEL_ID\",\n",
      "                \"value\": \"microsoft/phi-2\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"CACHE_DIR\",\n",
      "                \"value\": \"s3://mlrun/projects/llm-serving-johannes/artifacts/cache\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"S3_ENDPOINT_URL\",\n",
      "                \"value\": \"http://192.168.1.184:30080\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"AWS_ACCESS_KEY_ID\",\n",
      "                \"value\": \"4pab9IwyiPbBJqn0KUgV\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"AWS_SECRET_ACCESS_KEY\",\n",
      "                \"value\": \"VX2SPLubH7fp10RS2Hi1AE9UyV06XZvWuLMhMdeN\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"MLRUN_DEFAULT_PROJECT\",\n",
      "                \"value\": \"llm-serving-johannes\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"MLRUN_DBPATH\",\n",
      "                \"value\": \"http://mlrun-api:8080\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"MLRUN_NAMESPACE\",\n",
      "                \"value\": \"mlrun\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"MLRUN_AUTH_SESSION\",\n",
      "                \"value\": \"$generate\"\n",
      "            }\n",
      "        ],\n",
      "        \"resources\": {\n",
      "            \"limits\": {\n",
      "                \"nvidia.com/gpu\": \"1\"\n",
      "            },\n",
      "            \"requests\": {\n",
      "                \"cpu\": \"25m\",\n",
      "                \"memory\": \"1Mi\"\n",
      "            }\n",
      "        },\n",
      "        \"image\": \"registry-service.mlrun.svc.cluster.local/processor-llm-serving-johannes-serve-llm:latest\",\n",
      "        \"imageHash\": \"1753961755787742158\",\n",
      "        \"minReplicas\": 1,\n",
      "        \"maxReplicas\": 1,\n",
      "        \"triggers\": {\n",
      "            \"http\": {\n",
      "                \"class\": \"\",\n",
      "                \"kind\": \"http\",\n",
      "                \"name\": \"http\",\n",
      "                \"numWorkers\": 1,\n",
      "                \"annotations\": {\n",
      "                    \"nginx.ingress.kubernetes.io/proxy-connect-timeout\": \"150\",\n",
      "                    \"nginx.ingress.kubernetes.io/proxy-read-timeout\": \"150\",\n",
      "                    \"nginx.ingress.kubernetes.io/proxy-send-timeout\": \"150\"\n",
      "                },\n",
      "                \"workerAvailabilityTimeoutMilliseconds\": 120000,\n",
      "                \"attributes\": {\n",
      "                    \"ingresses\": {},\n",
      "                    \"serviceType\": \"NodePort\"\n",
      "                },\n",
      "                \"maxWorkers\": 1\n",
      "            }\n",
      "        },\n",
      "        \"version\": -1,\n",
      "        \"alias\": \"latest\",\n",
      "        \"build\": {\n",
      "            \"functionSourceCode\": \"aW1wb3J0IG9zCmltcG9ydCBqc29uCgppbXBvcnQgbWxydW4KaW1wb3J0IHRvcmNoCmZyb20gdHJhbnNmb3JtZXJzIGltcG9ydCAoCiAgICBBdXRvTW9kZWxGb3JDYXVzYWxMTSwKICAgIEF1dG9Ub2tlbml6ZXIsCiAgICBTdG9wcGluZ0NyaXRlcmlhLAogICAgU3RvcHBpbmdDcml0ZXJpYUxpc3QsCiAgICBwaXBlbGluZSwKICAgIHNldF9zZWVkLAopCgpkZWYgaW5pdF9jb250ZXh0KGNvbnRleHQpOgogICAgbW9kZWxfaWQgPSBvcy5lbnZpcm9uWyJNT0RFTF9JRCJdCiAgICBjYWNoZV9kaXIgPSBvcy5lbnZpcm9uWyJDQUNIRV9ESVIiXQoKICAgICMgSW5pdGlhbGl6ZSBIRiBtb2RlbHMKICAgIHRva2VuaXplciA9IEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkLCBjYWNoZV9kaXI9Y2FjaGVfZGlyKQogICAgbG1fbW9kZWwgPSBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQsIGNhY2hlX2Rpcj1jYWNoZV9kaXIpCgogICAgIyBIRiB0ZXh0IGdlbmVyYXRpb24gcGlwZWxpbmUKICAgIHBpcGUgPSBwaXBlbGluZSgKICAgICAgICAidGV4dC1nZW5lcmF0aW9uIiwKICAgICAgICBtb2RlbD1sbV9tb2RlbCwKICAgICAgICB0b2tlbml6ZXI9dG9rZW5pemVyLAogICAgICAgIG1heF9uZXdfdG9rZW5zPTUxMiwKICAgICAgICBkZXZpY2VfbWFwPSJhdXRvIgogICAgKQoKICAgIHNldGF0dHIoY29udGV4dC51c2VyX2RhdGEsICJwaXBlIiwgcGlwZSkKCgpkZWYgaW52b2tlX2xsbShjb250ZXh0LCBldmVudCk6CiAgICBldmVudF9qc29uID0ganNvbi5sb2FkcyhldmVudC5ib2R5KQogICAgcHJpbnQoZiJSZWNlaXZlZCBldmVudDoge2V2ZW50X2pzb259Iik=\",\n",
      "            \"registry\": \"registry-service.mlrun.svc.cluster.local\",\n",
      "            \"baseImage\": \"registry-service.mlrun.svc.cluster.local/mlrun/mlrun-gpu:1.9.1-py39\",\n",
      "            \"commands\": [\n",
      "                \"python -m pip install 'mlrun[complete]==1.9.1' transformers==4.41.2 tensorflow==2.16.1 torch\"\n",
      "            ],\n",
      "            \"codeEntryType\": \"sourceCode\",\n",
      "            \"timestamp\": 1753961982\n",
      "        },\n",
      "        \"runRegistry\": \"registry-service.mlrun.svc.cluster.local\",\n",
      "        \"imagePullSecrets\": \"registry-credentials\",\n",
      "        \"platform\": {},\n",
      "        \"readinessTimeoutSeconds\": 1200,\n",
      "        \"serviceType\": \"NodePort\",\n",
      "        \"securityContext\": {},\n",
      "        \"disableDefaultHTTPTrigger\": false,\n",
      "        \"eventTimeout\": \"\",\n",
      "        \"preemptionMode\": \"prevent\"\n",
      "    },\n",
      "    \"PlatformConfig\": null\n",
      "}\"}\n",
      "25.07.31 11:46:20.427 \u001b[34m(I)\u001b[0m \u001b[37mcessor.healthcheck.server\u001b[0m Listening {\"listenAddress\": \":8082\"}\n",
      "25.07.31 11:46:20.427 \u001b[32m(D)\u001b[0m \u001b[37m           processor.http\u001b[0m Creating worker pool {\"num\": 1}\n",
      "25.07.31 11:46:20.427 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Creating listener socket {\"path\": \"/tmp/nuclio-rpc-d25lf304nfss73bm3tc0.sock\"}\n",
      "25.07.31 11:46:20.428 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Creating listener socket {\"path\": \"/tmp/nuclio-rpc-d25lf304nfss73bm3tcg.sock\"}\n",
      "25.07.31 11:46:20.428 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Using Python wrapper script path {\"path\": \"/opt/nuclio/_nuclio_wrapper.py\"}\n",
      "25.07.31 11:46:20.428 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Using Python handler {\"handler\": \"06-01-serving-nuclio:invoke_llm\"}\n",
      "25.07.31 11:46:20.428 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Using Python executable {\"path\": \"/opt/conda/bin/python3\"}\n",
      "25.07.31 11:46:20.428 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Setting PYTHONPATH {\"value\": \"PYTHONPATH=/opt/nuclio\"}\n",
      "25.07.31 11:46:20.428 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Running wrapper {\"command\": \"/opt/conda/bin/python3 -u /opt/nuclio/_nuclio_wrapper.py --handler 06-01-serving-nuclio:invoke_llm --event-socket-path /tmp/nuclio-rpc-d25lf304nfss73bm3tc0.sock --control-socket-path /tmp/nuclio-rpc-d25lf304nfss73bm3tcg.sock --platform-kind kube --namespace mlrun --worker-id 0 --trigger-kind http --trigger-name http --decode-event-strings\"}\n",
      "2025-07-31 11:46:23.081076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-31 11:46:23.633146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "25.07.31 11:46:24.848 \u001b[34m(I)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Wrapper connected {\"wid\": 0, \"pid\": 47}\n",
      "25.07.31 11:46:24.848 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Creating control connection {\"wid\": 0}\n",
      "25.07.31 11:46:24.848 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Control connection created {\"wid\": 0}\n",
      "25.07.31 11:46:24.848 \u001b[32m(D)\u001b[0m \u001b[37msor.http.w0.python.logger\u001b[0m Waiting for start\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "> 2025-07-31 13:59:44,538 [error] Nuclio function failed to deploy: {\"function_state\":\"unhealthy\"}\n"
     ]
    },
    {
     "ename": "RunError",
     "evalue": "Function serve-llm deployment failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRunError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# build the function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#project.build_function(function='serve-llm')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# deploy the function\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#project.deploy_function(serve_func)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m serve_func \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserve-llm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/local-mlrun/lib/python3.9/site-packages/mlrun/projects/project.py:4241\u001b[0m, in \u001b[0;36mMlrunProject.deploy_function\u001b[0;34m(self, function, models, env, tag, verbose, builder_env, mock)\u001b[0m\n\u001b[1;32m   4221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeploy_function\u001b[39m(\n\u001b[1;32m   4222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4223\u001b[0m     function: typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mstr\u001b[39m, mlrun\u001b[38;5;241m.\u001b[39mruntimes\u001b[38;5;241m.\u001b[39mBaseRuntime],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4229\u001b[0m     mock: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[DeployStatus, PipelineNodeWrapper]:\n\u001b[1;32m   4231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"deploy real-time (nuclio based) functions\u001b[39;00m\n\u001b[1;32m   4232\u001b[0m \n\u001b[1;32m   4233\u001b[0m \u001b[38;5;124;03m    :param function:    name of the function (in the project) or function object\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4239\u001b[0m \u001b[38;5;124;03m    :param mock:        deploy mock server vs a real Nuclio function (for local simulations)\u001b[39;00m\n\u001b[1;32m   4240\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeploy_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4244\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuilder_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/local-mlrun/lib/python3.9/site-packages/mlrun/projects/operations.py:418\u001b[0m, in \u001b[0;36mdeploy_function\u001b[0;34m(function, models, env, tag, verbose, builder_env, project_object, mock)\u001b[0m\n\u001b[1;32m    411\u001b[0m     function\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DeployStatus(\n\u001b[1;32m    413\u001b[0m         state\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    414\u001b[0m         outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: function\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mname},\n\u001b[1;32m    415\u001b[0m         function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[0;32m--> 418\u001b[0m address \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilder_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# return object with the same outputs as the KFP op (allow using the same pipeline)\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DeployStatus(\n\u001b[1;32m    421\u001b[0m     state\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mstate,\n\u001b[1;32m    422\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: address, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: function\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mnuclio_name},\n\u001b[1;32m    423\u001b[0m     function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    424\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/local-mlrun/lib/python3.9/site-packages/mlrun/runtimes/nuclio/function.py:658\u001b[0m, in \u001b[0;36mRemoteRuntime.deploy\u001b[0;34m(self, project, tag, verbose, auth_info, builder_env, force_build)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_credentials_from_remote_build(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# when a function is deployed, we wait for it to be ready by default\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# this also means that the function object will be updated with the function status\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_function_deployment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# check if there are any background tasks related to creating model endpoints\u001b[39;00m\n\u001b[1;32m    660\u001b[0m model_endpoints_creation_background_tasks \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    661\u001b[0m     mlrun\u001b[38;5;241m.\u001b[39mcommon\u001b[38;5;241m.\u001b[39mschemas\u001b[38;5;241m.\u001b[39mBackgroundTaskList(\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground_tasks\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground_tasks\u001b[39m\u001b[38;5;124m\"\u001b[39m: []})\n\u001b[1;32m    663\u001b[0m     )\u001b[38;5;241m.\u001b[39mbackground_tasks\n\u001b[1;32m    664\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/local-mlrun/lib/python3.9/site-packages/mlrun/runtimes/nuclio/function.py:721\u001b[0m, in \u001b[0;36mRemoteRuntime._wait_for_function_deployment\u001b[0;34m(self, db, verbose)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    720\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNuclio function failed to deploy\u001b[39m\u001b[38;5;124m\"\u001b[39m, function_state\u001b[38;5;241m=\u001b[39mstate)\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RunError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m deployment failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRunError\u001b[0m: Function serve-llm deployment failed"
     ]
    }
   ],
   "source": [
    "# build the function\n",
    "#project.build_function(function='serve-llm')\n",
    "\n",
    "# deploy the function\n",
    "#project.deploy_function(serve_func)\n",
    "serve_func = project.deploy_function(function=\"serve-llm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
