{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06-01 : LLM Serving\n",
    "\n",
    "A basic test to serve a LLM as a function.\n",
    "\n",
    "## Refrences\n",
    "\n",
    "- [Deploying an LLM using MLRun](https://docs.mlrun.org/en/v1.7.2/tutorials/genai_01_basic_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTTPRunDB('http://dragon.local:30070')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the API server URL\n",
    "mlrun.get_run_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"microsoft/phi-2\" # the model ID to use\n",
    "project_name = \"llm-serving\" # the project name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-28 15:15:54,646 [info] Project loaded successfully: {\"project_name\":\"llm-serving-johannes\"}\n",
      "Full project name: llm-serving-johannes\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name,\n",
    "    user_project=True)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model Cache Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: s3://mlrun/projects/llm-serving-johannes/artifacts/cache\n"
     ]
    }
   ],
   "source": [
    "# the cache directory for the model\n",
    "CACHE_DIR = mlrun.mlconf.artifact_path\n",
    "CACHE_DIR = (\n",
    "    CACHE_DIR.replace(\"v3io://\", \"/v3io\").replace(\"{{run.project}}\", project.name)\n",
    "    + \"/cache\"\n",
    ")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-28 15:15:54,717 [warning] Adding HTTP trigger despite the default HTTP trigger creation being disabled\n"
     ]
    }
   ],
   "source": [
    "# requirements for the function\n",
    "requirements = [\n",
    "    \"transformers==4.41.2\",\n",
    "    \"tensorflow==2.16.1\",\n",
    "    \"torch\"\n",
    "]\n",
    "\n",
    "# create the function to serve the model \n",
    "serve_func = project.set_function(\n",
    "    name=\"serve-llm\",\n",
    "    func=\"src/06-01_serving.py\",\n",
    "    image=\"dragon:6500/mlrun/mlrun-gpu:1.7.2\",\n",
    "    kind=\"nuclio\",\n",
    "    handler=\"invoke_llm\",\n",
    "    requirements=requirements\n",
    ").apply(mlrun.auto_mount())\n",
    "\n",
    "# set the environment variables for the function\n",
    "serve_func.set_envs(env_vars={\n",
    "    \"MODEL_ID\": MODEL_ID, \n",
    "    \"CACHE_DIR\": CACHE_DIR\n",
    "})\n",
    "\n",
    "# Since the model is stored in memory, use only 1 replica and and one worker\n",
    "# Since this is running on CPU only, inference might take ~1 minute (increasing timeout)\n",
    "serve_func.spec.min_replicas = 1\n",
    "serve_func.spec.max_replicas = 1\n",
    "serve_func.with_http(worker_timeout=120, gateway_timeout=150, workers=1)\n",
    "serve_func.set_config(\"spec.readinessTimeoutSeconds\", 1200)\n",
    "\n",
    "# set gpu resources for the function\n",
    "serve_func.with_limits(gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-28 15:15:54,734 [info] Starting remote function deploy\n",
      "2025-07-28 13:15:55  (info) Deploying function\n",
      "2025-07-28 13:15:55  (info) Building\n",
      "2025-07-28 13:15:55  (info) Staging files and preparing base images\n",
      "2025-07-28 13:15:55  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2025-07-28 13:15:55  (info) Building processor image\n",
      "Failed to deploy. Details:\n",
      "\n",
      "Error - Job failed. Job logs:\n",
      "error checking push permissions -- make sure you entered the correct tag name, and that you are authenticated correctly, and try again: checking push permission for \"dragon:6500/nuclio/processor-llm-serving-johannes-serve-llm:latest\": creating push check transport for dragon:6500 failed: Get \"https://dragon:6500/v2/\": http: server gave HTTP response to HTTPS client\n",
      "    /nuclio/pkg/processor/build/builder.go:276\n",
      "\n",
      "Call stack:\n",
      "Failed to build processor image\n",
      "    /nuclio/pkg/processor/build/builder.go:276\n",
      "\n",
      "> 2025-07-28 15:16:15,099 [error] Nuclio function failed to deploy: {\"function_state\":\"error\"}\n"
     ]
    },
    {
     "ename": "RunError",
     "evalue": "Function serve-llm deployment failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRunError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# build the function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#project.build_function(function='serve-llm')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# deploy the function\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#project.deploy_function(serve_func)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m serve_func \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserve-llm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/local-mlrun/lib/python3.9/site-packages/mlrun/projects/project.py:3760\u001b[0m, in \u001b[0;36mMlrunProject.deploy_function\u001b[0;34m(self, function, models, env, tag, verbose, builder_env, mock)\u001b[0m\n\u001b[1;32m   3740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeploy_function\u001b[39m(\n\u001b[1;32m   3741\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3742\u001b[0m     function: typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mstr\u001b[39m, mlrun\u001b[38;5;241m.\u001b[39mruntimes\u001b[38;5;241m.\u001b[39mBaseRuntime],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3748\u001b[0m     mock: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3749\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[DeployStatus, PipelineNodeWrapper]:\n\u001b[1;32m   3750\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"deploy real-time (nuclio based) functions\u001b[39;00m\n\u001b[1;32m   3751\u001b[0m \n\u001b[1;32m   3752\u001b[0m \u001b[38;5;124;03m    :param function:    name of the function (in the project) or function object\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[38;5;124;03m    :param mock:        deploy mock server vs a real Nuclio function (for local simulations)\u001b[39;00m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeploy_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3763\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuilder_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/local-mlrun/lib/python3.9/site-packages/mlrun/projects/operations.py:418\u001b[0m, in \u001b[0;36mdeploy_function\u001b[0;34m(function, models, env, tag, verbose, builder_env, project_object, mock)\u001b[0m\n\u001b[1;32m    411\u001b[0m     function\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DeployStatus(\n\u001b[1;32m    413\u001b[0m         state\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    414\u001b[0m         outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: function\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mname},\n\u001b[1;32m    415\u001b[0m         function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[0;32m--> 418\u001b[0m address \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilder_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# return object with the same outputs as the KFP op (allow using the same pipeline)\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DeployStatus(\n\u001b[1;32m    421\u001b[0m     state\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mstate,\n\u001b[1;32m    422\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: address, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: function\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mnuclio_name},\n\u001b[1;32m    423\u001b[0m     function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    424\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/local-mlrun/lib/python3.9/site-packages/mlrun/runtimes/nuclio/function.py:602\u001b[0m, in \u001b[0;36mRemoteRuntime.deploy\u001b[0;34m(self, project, tag, verbose, auth_info, builder_env, force_build)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_credentials_from_remote_build(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# when a function is deployed, we wait for it to be ready by default\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# this also means that the function object will be updated with the function status\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_function_deployment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enrich_command_from_status()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/local-mlrun/lib/python3.9/site-packages/mlrun/runtimes/nuclio/function.py:653\u001b[0m, in \u001b[0;36mRemoteRuntime._wait_for_function_deployment\u001b[0;34m(self, db, verbose)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    652\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNuclio function failed to deploy\u001b[39m\u001b[38;5;124m\"\u001b[39m, function_state\u001b[38;5;241m=\u001b[39mstate)\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RunError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m deployment failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRunError\u001b[0m: Function serve-llm deployment failed"
     ]
    }
   ],
   "source": [
    "# build the function\n",
    "#project.build_function(function='serve-llm')\n",
    "\n",
    "# deploy the function\n",
    "#project.deploy_function(serve_func)\n",
    "serve_func = project.deploy_function(function=\"serve-llm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
