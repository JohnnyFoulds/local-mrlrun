{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24c37cb",
   "metadata": {},
   "source": [
    "# 06-05 : Download Model\n",
    "\n",
    "Download a LLM Model from Hugging Face and store it as an artifact in MLRun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b736afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "695226d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7636c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 20:12:50 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from functions.vllm_model_server import VLLMModelServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d4e9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTTPRunDB('http://dragon.local:30070')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the API server URL\n",
    "mlrun.get_run_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a67aed",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee45fc7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepseek-r1-distill-qwen-14b-awq\"\n",
    "MODEL_ID = f\"casperhansen/{MODEL_NAME}\"\n",
    "MODEL_PATH = f\"'/data/.cache/huggingface/{MODEL_NAME}\"\n",
    "\n",
    "project_name = \"test-vllm-integration\" # the project name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88a1fb",
   "metadata": {},
   "source": [
    "### 1.1 Create The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaeda365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-11 20:12:51,026 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "Full project name: test-vllm-integration\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name,\n",
    "    user_project=False)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ab8b2",
   "metadata": {},
   "source": [
    "## 2. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the context\n",
    "context = mlrun.get_or_create_ctx(project_name)\n",
    "\n",
    "# create the model server function\n",
    "vllm = VLLMModelServer(\n",
    "    context=context,\n",
    "    name=MODEL_NAME,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_name=MODEL_ID,\n",
    ")\n",
    "\n",
    "# download the model\n",
    "vllm.store_model()\n",
    "\n",
    "# get the model artifact\n",
    "model_artifact = project.get_artifact(MODEL_NAME)\n",
    "print(f\"Model Artifact Uri: {model_artifact.uri}\")\n",
    "print(f\"Model Artifact S3: {model_artifact.target_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f865de",
   "metadata": {},
   "source": [
    "## 2. vLLM Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8d81a",
   "metadata": {},
   "source": [
    "### 2.1 CLI Test\n",
    "\n",
    "Start the vLLM server to serve the model:\n",
    "\n",
    "```bash\n",
    "vllm serve s3://mlrun/projects/test-vllm-integration/artifacts/deepseek-r1-distill-qwen-14b-awq/ --load-format runai_streamer --max-model-len 32768\n",
    "```\n",
    "\n",
    "Simple Promt:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "    \"prompt\": \"The chees moon?\",\n",
    "    \"max_tokens\": 50\n",
    "}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b43e1",
   "metadata": {},
   "source": [
    "### 2.2 Code Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33875dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model URI: s3://mlrun/projects/test-vllm-integration/artifacts/Tiny-LLM/\n"
     ]
    }
   ],
   "source": [
    "#model_artifact = project.get_artifact(MODEL_NAME)\n",
    "model_artifact = project.get_artifact(\"Tiny-LLM\")\n",
    "model_uri = model_artifact.target_path\n",
    "\n",
    "print(f\"Model URI: {model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "?LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17340f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 20:46:22 [config.py:3440] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 08-11 20:46:22 [config.py:1604] Using max model len 1024\n",
      "INFO 08-11 20:46:22 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-11 20:46:22 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-11 20:46:22 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/tmp/tmpipxqny24', speculative_config=None, tokenizer='/tmp/tmpipxqny24', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=runai_streamer, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=s3://mlrun/projects/test-vllm-integration/artifacts/Tiny-LLM/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-11 20:46:24 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-11 20:46:24 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.\n",
      "INFO 08-11 20:46:24 [gpu_model_runner.py:1843] Starting to load model /tmp/tmpipxqny24...\n",
      "INFO 08-11 20:46:24 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-11 20:46:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "[RunAI Streamer] CPU Buffer size: 8 Bytes for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Tiny-LLM/model.safetensors']\n",
      "[RunAI Streamer] CPU Buffer size: 1.2 KiB for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Tiny-LLM/model.safetensors']\n",
      "[RunAI Streamer] CPU Buffer size: 24.8 MiB for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Tiny-LLM/model.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d852e18ac24f1cbe762350ac36ee75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors using Runai Model Streamer:   0% Completed | 0/12 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RunAI Streamer] Overall time to stream 24.8 MiB of all files: 0.09s, 276.3 MiB/s\n",
      "INFO 08-11 20:46:25 [gpu_model_runner.py:1892] Model loading took 0.0247 GiB and 0.202080 seconds\n",
      "INFO 08-11 20:46:25 [backends.py:530] Using cache directory: /home/johnny/.cache/vllm/torch_compile_cache/6842510834/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-11 20:46:25 [backends.py:541] Dynamo bytecode transform time: 0.54 s\n",
      "INFO 08-11 20:46:27 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 08-11 20:46:28 [backends.py:215] Compiling a graph for dynamic shape takes 2.38 s\n",
      "INFO 08-11 20:46:28 [monitor.py:34] torch.compile takes 2.92 s in total\n",
      "INFO 08-11 20:46:29 [gpu_worker.py:255] Available KV cache memory: 20.96 GiB\n",
      "INFO 08-11 20:46:29 [kv_cache_utils.py:833] GPU KV cache size: 58,609,536 tokens\n",
      "INFO 08-11 20:46:29 [kv_cache_utils.py:837] Maximum concurrency for 1,024 tokens per request: 57235.88x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:00<00:00, 658.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 20:46:30 [gpu_model_runner.py:2485] Graph capturing finished in 0 secs, took 0.08 GiB\n",
      "INFO 08-11 20:46:30 [core.py:193] init engine (profile, create kv cache, warmup model) took 4.94 seconds\n",
      "ERROR 08-11 20:46:30 [core.py:632] EngineCore failed to start.\n",
      "ERROR 08-11 20:46:30 [core.py:632] Traceback (most recent call last):\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
      "ERROR 08-11 20:46:30 [core.py:632]     hf_hub_download(\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "ERROR 08-11 20:46:30 [core.py:632]     validate_repo_id(arg_value)\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "ERROR 08-11 20:46:30 [core.py:632]     raise HFValidationError(\n",
      "ERROR 08-11 20:46:30 [core.py:632] huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/tmp/tmpipxqny24'. Use `repo_type` argument if needed.\n",
      "ERROR 08-11 20:46:30 [core.py:632] \n",
      "ERROR 08-11 20:46:30 [core.py:632] During handling of the above exception, another exception occurred:\n",
      "ERROR 08-11 20:46:30 [core.py:632] \n",
      "ERROR 08-11 20:46:30 [core.py:632] Traceback (most recent call last):\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 623, in run_engine_core\n",
      "ERROR 08-11 20:46:30 [core.py:632]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 441, in __init__\n",
      "ERROR 08-11 20:46:30 [core.py:632]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 93, in __init__\n",
      "ERROR 08-11 20:46:30 [core.py:632]     self.structured_output_manager = StructuredOutputManager(vllm_config)\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/structured_output/__init__.py\", line 51, in __init__\n",
      "ERROR 08-11 20:46:30 [core.py:632]     self.tokenizer = init_tokenizer_from_configs(\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group.py\", line 111, in init_tokenizer_from_configs\n",
      "ERROR 08-11 20:46:30 [core.py:632]     return TokenizerGroup(\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group.py\", line 24, in __init__\n",
      "ERROR 08-11 20:46:30 [core.py:632]     self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py\", line 259, in get_tokenizer\n",
      "ERROR 08-11 20:46:30 [core.py:632]     raise e\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py\", line 238, in get_tokenizer\n",
      "ERROR 08-11 20:46:30 [core.py:632]     tokenizer = AutoTokenizer.from_pretrained(\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 1049, in from_pretrained\n",
      "ERROR 08-11 20:46:30 [core.py:632]     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 881, in get_tokenizer_config\n",
      "ERROR 08-11 20:46:30 [core.py:632]     resolved_config_file = cached_file(\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 321, in cached_file\n",
      "ERROR 08-11 20:46:30 [core.py:632]     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 531, in cached_files\n",
      "ERROR 08-11 20:46:30 [core.py:632]     resolved_files = [\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 532, in <listcomp>\n",
      "ERROR 08-11 20:46:30 [core.py:632]     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 144, in _get_cache_file_to_return\n",
      "ERROR 08-11 20:46:30 [core.py:632]     resolved_file = try_to_load_from_cache(\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "ERROR 08-11 20:46:30 [core.py:632]     validate_repo_id(arg_value)\n",
      "ERROR 08-11 20:46:30 [core.py:632]   File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "ERROR 08-11 20:46:30 [core.py:632]     raise HFValidationError(\n",
      "ERROR 08-11 20:46:30 [core.py:632] huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/tmp/tmpipxqny24'. Use `repo_type` argument if needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/tmp/tmpipxqny24'. Use `repo_type` argument if needed.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 636, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 623, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 441, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 93, in __init__\n",
      "    self.structured_output_manager = StructuredOutputManager(vllm_config)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/structured_output/__init__.py\", line 51, in __init__\n",
      "    self.tokenizer = init_tokenizer_from_configs(\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group.py\", line 111, in init_tokenizer_from_configs\n",
      "    return TokenizerGroup(\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group.py\", line 24, in __init__\n",
      "    self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py\", line 259, in get_tokenizer\n",
      "    raise e\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py\", line 238, in get_tokenizer\n",
      "    tokenizer = AutoTokenizer.from_pretrained(\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 1049, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 881, in get_tokenizer_config\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 321, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 531, in cached_files\n",
      "    resolved_files = [\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 532, in <listcomp>\n",
      "    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/transformers/utils/hub.py\", line 144, in _get_cache_file_to_return\n",
      "    resolved_file = try_to_load_from_cache(\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/home/johnny/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/tmp/tmpipxqny24'. Use `repo_type` argument if needed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVLLM_CI_USE_S3\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m----> 4\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#max_model_len=32768,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunai_streamer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_chunked_prefill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/entrypoints/llm.py:273\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    244\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    245\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    270\u001b[0m )\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/engine/llm_engine.py:497\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[1;32m    495\u001b[0m     engine_cls \u001b[38;5;241m=\u001b[39m V1LLMEngine\n\u001b[0;32m--> 497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:126\u001b[0m, in \u001b[0;36mLLMEngine.from_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_vllm_config\u001b[39m(\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMEngine\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:103\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor \u001b[38;5;241m=\u001b[39m OutputProcessor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m    100\u001b[0m                                         log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mmodel_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:77\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient\u001b[38;5;241m.\u001b[39mmake_async_mp_client(\n\u001b[1;32m     74\u001b[0m         vllm_config, executor_class, log_stats)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:514\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    513\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:408\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_update_address \u001b[38;5;241m=\u001b[39m client_addresses\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats_update_address\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m launch_core_engines(vllm_config, executor_class,\n\u001b[1;32m    409\u001b[0m                              log_stats) \u001b[38;5;28;01mas\u001b[39;00m (engine_manager,\n\u001b[1;32m    410\u001b[0m                                             coordinator,\n\u001b[1;32m    411\u001b[0m                                             addresses):\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mcoordinator \u001b[38;5;241m=\u001b[39m coordinator\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mengine_manager \u001b[38;5;241m=\u001b[39m engine_manager\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/utils.py:697\u001b[0m, in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/swan/miniconda3/envs/local-mlrun/lib/python3.10/site-packages/vllm/v1/engine/utils.py:750\u001b[0m, in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process\u001b[38;5;241m.\u001b[39mexitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    749\u001b[0m         finished[coord_process\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m coord_process\u001b[38;5;241m.\u001b[39mexitcode\n\u001b[0;32m--> 750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine core initialization failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee root cause above. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[1;32m    755\u001b[0m eng_identity, ready_msg_bytes \u001b[38;5;241m=\u001b[39m handshake_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "os.environ[\"VLLM_CI_USE_S3\"] = \"1\" \n",
    "\n",
    "# llm = LLM(\n",
    "#     model=model_uri,  \n",
    "#     tokenizer=model_uri,\n",
    "#     hf_config_path=model_uri,\n",
    "#     #max_model_len=32768,\n",
    "#     max_model_len=1024,\n",
    "#     trust_remote_code=True,\n",
    "#     load_format=\"runai_streamer\",\n",
    "#     enable_chunked_prefill=False\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
