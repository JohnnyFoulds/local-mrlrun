{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24c37cb",
   "metadata": {},
   "source": [
    "# 06-05 : Download Model\n",
    "\n",
    "Download a LLM Model from Hugging Face and store it as an artifact in MLRun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b736afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "695226d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7636c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import mlrun\n",
    "from IPython.display import Markdown, display\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from functions.vllm_model_server import VLLMModelServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d4e9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTTPRunDB('http://dragon.local:30070')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the API server URL\n",
    "mlrun.get_run_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a67aed",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e31423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"deepseek-r1-distill-qwen-14b-awq\"\n",
    "#MODEL_ID = f\"casperhansen/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee45fc7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "MODEL_ID = f\"TheBloke/{MODEL_NAME}\"\n",
    "MODEL_PATH = f\"'/data/.cache/huggingface/{MODEL_NAME}\"\n",
    "\n",
    "project_name = \"test-vllm-integration\" # the project name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88a1fb",
   "metadata": {},
   "source": [
    "### 1.1 Create The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaeda365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-11 23:31:30,245 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "Full project name: test-vllm-integration\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name,\n",
    "    user_project=False)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ab8b2",
   "metadata": {},
   "source": [
    "## 2. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252e9282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-11 23:31:30,259 [info] Logging run results to: http://dragon.local:30070\n",
      "> 2025-08-11 23:31:30,279 [info] Storing model TheBloke/Mistral-7B-Instruct-v0.2-AWQ in project test-vllm-integration\n",
      "> 2025-08-11 23:31:30,279 [info] Downloading model TheBloke/Mistral-7B-Instruct-v0.2-AWQ to '/data/.cache/huggingface/Mistral-7B-Instruct-v0.2-AWQ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4ecf9f035d4a7881ab7b10a503fa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bde67f998948db93fba3199bf208dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6dd5243ea04c22b76d12854e58d765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf5089f34644887b0ac87a7e750e336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fbe2b76b274d1d99307c1540695945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47220d2f4724122bcc5effa9365e998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quant_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e715a7a072864f1d95b84a652b5ae2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba729cf161e472e8157008ea86c3e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92f4cf89b424db6a58beb3294b35adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a74c7d9f0c14fc292b9fe25c45dfb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f593f7c0f2e41c29bdbe97128ab9ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-11 23:38:35,612 [info] Deleting cache directory: '/data/.cache/huggingface/Mistral-7B-Instruct-v0.2-AWQ/.cache\n",
      "> 2025-08-11 23:38:35,614 [info] Model TheBloke/Mistral-7B-Instruct-v0.2-AWQ downloaded successfully to: '/data/.cache/huggingface/Mistral-7B-Instruct-v0.2-AWQ\n",
      "> 2025-08-11 23:38:35,745 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "> 2025-08-11 23:38:35,746 [info] Logging model TheBloke/Mistral-7B-Instruct-v0.2-AWQ to project test-vllm-integration\n",
      "> 2025-08-11 23:38:54,477 [info] Model TheBloke/Mistral-7B-Instruct-v0.2-AWQ logged successfully to: store://artifacts/test-vllm-integration/Mistral-7B-Instruct-v0.2-AWQ#0@1dce1e31-27eb-471f-a8c4-46a2d3bcf0ec^89e1a5c62d8ff3d26e4fa12d7b4e4918be9b0c55\n",
      "> 2025-08-11 23:38:54,478 [info] Deleting local model files at '/data/.cache/huggingface/Mistral-7B-Instruct-v0.2-AWQ\n",
      "> 2025-08-11 23:38:55,063 [info] Model TheBloke/Mistral-7B-Instruct-v0.2-AWQ stored successfully.\n",
      "Model Artifact Uri: store://artifacts/test-vllm-integration/Mistral-7B-Instruct-v0.2-AWQ#0:latest@1dce1e31-27eb-471f-a8c4-46a2d3bcf0ec^89e1a5c62d8ff3d26e4fa12d7b4e4918be9b0c55\n",
      "Model Artifact S3: s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/\n"
     ]
    }
   ],
   "source": [
    "# get the context\n",
    "context = mlrun.get_or_create_ctx(project_name)\n",
    "\n",
    "# create the model server function\n",
    "vllm = VLLMModelServer(\n",
    "    context=context,\n",
    "    name=MODEL_NAME,\n",
    "    model_path=MODEL_PATH,\n",
    "    model_name=MODEL_ID,\n",
    ")\n",
    "\n",
    "# download the model\n",
    "vllm.store_model()\n",
    "\n",
    "# get the model artifact\n",
    "model_artifact = project.get_artifact(MODEL_NAME)\n",
    "print(f\"Model Artifact Uri: {model_artifact.uri}\")\n",
    "print(f\"Model Artifact S3: {model_artifact.target_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f865de",
   "metadata": {},
   "source": [
    "## 2. vLLM Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8d81a",
   "metadata": {},
   "source": [
    "### 2.1 CLI Test\n",
    "\n",
    "Start the vLLM server to serve the model:\n",
    "\n",
    "```bash\n",
    "vllm serve s3://mlrun/projects/test-vllm-integration/artifacts/deepseek-r1-distill-qwen-14b-awq/ --load-format runai_streamer --max-model-len 32768\n",
    "```\n",
    "\n",
    "Simple Promt:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "    \"prompt\": \"The chees moon?\",\n",
    "    \"max_tokens\": 50\n",
    "}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b43e1",
   "metadata": {},
   "source": [
    "### 2.2 Code Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe61ebe",
   "metadata": {},
   "source": [
    "#### 2.1.1 Get The Model Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33875dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model URI: s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/\n"
     ]
    }
   ],
   "source": [
    "model_artifact = project.get_artifact(MODEL_NAME)\n",
    "#model_artifact = project.get_artifact(\"Tiny-LLM\")\n",
    "model_uri = model_artifact.target_path\n",
    "\n",
    "print(f\"Model URI: {model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c42e8",
   "metadata": {},
   "source": [
    "#### 2.1.2 Download The Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8472f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.gitattributes', 'README.md', 'config.json', 'generation_config.json', 'model.safetensors', 'quant_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "# get the data item\n",
    "model_data_item = mlrun.get_dataitem(model_artifact.uri)\n",
    "print(model_data_item.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde3954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config.json to /tmp/vllm_tokenizer_lzpe2bat\n",
      "Downloading generation_config.json to /tmp/vllm_tokenizer_lzpe2bat\n",
      "Downloading special_tokens_map.json to /tmp/vllm_tokenizer_lzpe2bat\n",
      "Downloading tokenizer.json to /tmp/vllm_tokenizer_lzpe2bat\n",
      "Downloading tokenizer.model to /tmp/vllm_tokenizer_lzpe2bat\n",
      "Downloading tokenizer_config.json to /tmp/vllm_tokenizer_lzpe2bat\n",
      "Tokenizer files downloaded to: /tmp/vllm_tokenizer_lzpe2bat\n"
     ]
    }
   ],
   "source": [
    "def download_tokenizer(model_artifact) -> str:\n",
    "    \"\"\"\n",
    "    Download the tokenizer from the model data item.\n",
    "    \"\"\"\n",
    "    # tokenizer files normally needed:\n",
    "    tokenizer_files = [\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer.model\",         # some models have one or the other\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"config.json\",              # often needed too\n",
    "        \"generation_config.json\",   # optional, if exists\n",
    "    ]\n",
    "    \n",
    "    # create a temporary directory to store the tokenizer files\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"vllm_tokenizer_\")\n",
    "\n",
    "    # get the files in the data item\n",
    "    data_item = mlrun.get_dataitem(model_artifact.uri)\n",
    "    data_item_files = data_item.listdir()\n",
    "\n",
    "    # download tokenizer-related files\n",
    "    for filename in data_item_files:\n",
    "        # if the file is not in the tokenizer files, skip it\n",
    "        if filename not in tokenizer_files:\n",
    "            continue\n",
    "        \n",
    "        # download the file to the temporary directory\n",
    "        print(f\"Downloading {filename} to {temp_dir}\")\n",
    "        data_item_file = mlrun.get_dataitem(f\"{model_data_item.url}{filename}\")\n",
    "        data_item_file.download(target_path=f\"{temp_dir}/{filename}\")\n",
    "\n",
    "\n",
    "    return temp_dir\n",
    "\n",
    "##   Download the tokenizer files\n",
    "tokenizer_dir = download_tokenizer(model_artifact)\n",
    "print(f\"Tokenizer files downloaded to: {tokenizer_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "107ca833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b17340f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 23:38:59 [config.py:1604] Using max model len 32768\n",
      "INFO 08-11 23:39:00 [awq_marlin.py:120] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "WARNING 08-11 23:39:00 [config.py:1084] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 08-11 23:39:01 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-11 23:39:01 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-11 23:39:01 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/tmp/tmpqlxcaiif', speculative_config=None, tokenizer='/tmp/vllm_tokenizer_lzpe2bat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=runai_streamer, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-11 23:39:02 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-11 23:39:02 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.\n",
      "INFO 08-11 23:39:02 [gpu_model_runner.py:1843] Starting to load model /tmp/tmpqlxcaiif...\n",
      "INFO 08-11 23:39:02 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-11 23:39:02 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "[RunAI Streamer] CPU Buffer size: 8 Bytes for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n",
      "[RunAI Streamer] CPU Buffer size: 82.0 KiB for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n",
      "[RunAI Streamer] CPU Buffer size: 3.9 GiB for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a781d5a0b04712b4752b298726324e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors using Runai Model Streamer:   0% Completed | 0/739 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RunAI Streamer] Overall time to stream 3.9 GiB of all files: 1.95s, 2.0 GiB/s\n",
      "Read throughput is 2.25 GB per second \n",
      "INFO 08-11 23:39:05 [gpu_model_runner.py:1892] Model loading took 3.8812 GiB and 2.226387 seconds\n",
      "INFO 08-11 23:39:10 [backends.py:530] Using cache directory: /home/johnny/.cache/vllm/torch_compile_cache/79a36ed55d/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-11 23:39:10 [backends.py:541] Dynamo bytecode transform time: 4.74 s\n",
      "INFO 08-11 23:39:12 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 08-11 23:39:30 [backends.py:215] Compiling a graph for dynamic shape takes 19.44 s\n",
      "INFO 08-11 23:39:37 [monitor.py:34] torch.compile takes 24.18 s in total\n",
      "INFO 08-11 23:39:38 [gpu_worker.py:255] Available KV cache memory: 16.38 GiB\n",
      "INFO 08-11 23:39:38 [kv_cache_utils.py:833] GPU KV cache size: 134,144 tokens\n",
      "INFO 08-11 23:39:38 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 4.09x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:07<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 23:39:46 [gpu_model_runner.py:2485] Graph capturing finished in 8 secs, took 0.83 GiB\n",
      "INFO 08-11 23:39:46 [core.py:193] init engine (profile, create kv cache, warmup model) took 41.08 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=model_uri,  \n",
    "    tokenizer=tokenizer_dir,\n",
    "    hf_config_path=tokenizer_dir,\n",
    "    #max_model_len=32768,\n",
    "    # max_model_len=1024,\n",
    "    trust_remote_code=True,\n",
    "    load_format=\"runai_streamer\",\n",
    "#     enable_chunked_prefill=False\n",
    "    quantization=\"AWQ\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5387680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbaa6bc48c24bb38ee2ce99a5db2b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7d6165a444427c84f2b385bbd5942e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## What is the capital of Congo?\n",
       "\n",
       " The capital city of the Republic of the Congo is Brazzaville. The Democratic Republic of Congo, also known as DRC or Zaire, has its capital in Kinshasa. These two countries are often confused due to their similar names and shared history. It is important to note that they are separate countries with distinct capitals. Brazzaville is located on the Congo River in the west of the country, while Kinshasa is situated on the Congo River in the southeast of DRC."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm.generate(\"What is the capital of Congo?\", sampling_params=sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    display(Markdown(f\"## {prompt}\\n\\n{generated_text}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
