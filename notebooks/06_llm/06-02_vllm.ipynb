{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06-02 : vLLM Ofline Batch Inference\n",
    "\n",
    "## References\n",
    "\n",
    "- [Quickstart](https://docs.vllm.ai/en/stable/getting_started/quickstart.html#offline-batched-inference)\n",
    "- [Deploying an LLM using MLRun](https://docs.mlrun.org/en/v1.9.1/tutorials/genai_01_basic_tutorial.html)\n",
    "- [Function hub](https://docs.mlrun.org/en/latest/runtimes/load-from-hub.html)\n",
    "- [Functions hub Repo](https://github.com/mlrun/functions)\n",
    "- [Building a docker image using a Dockerfile and then using it](https://docs.mlrun.org/en/v1.9.1/runtimes/images.html#building-a-docker-image-using-a-dockerfile-and-then-using-it)\n",
    "- [MLRun runtime images](https://docs.mlrun.org/en/v1.9.1/runtimes/images.html#mlrun-runtime-images)\n",
    "- [MLRun Images](https://github.com/mlrun/mlrun/tree/development/dockerfiles)\n",
    "- https://github.com/yaronha/mlrun/blob/36049e1f748e277bf380d44043129d408581a893/docs/runtimes/configuring-job-resources.md#cpu-gpu-and-memory-limits-for-user-jobs\n",
    "- [Select OpenAI or Ollama](https://github.com/liranbg/mlrun/blob/d61b4e2f37215ae9564aeb1dbbeee08c7f0c9d2c/docs/genai/development/working-with-rag.ipynb#L202)\n",
    "- [gpt-oss vLLM Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HTTPRunDB('http://dragon.local:30070')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the API server URL\n",
    "mlrun.get_run_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "project_name = \"llm-batch\" # the project name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-06 13:38:37,628 [info] Loading project from path: {\"path\":\"./\",\"project_name\":\"llm-batch\",\"user_project\":true}\n",
      "> 2025-08-06 13:38:37,655 [info] Project loaded successfully: {\"path\":\"./\",\"project_name\":\"llm-batch-johannes\",\"stored_in_db\":true}\n",
      "Full project name: llm-batch-johannes\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name,\n",
    "    user_project=True)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model Cache Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: s3://mlrun/projects/llm-batch-johannes/artifacts/cache\n"
     ]
    }
   ],
   "source": [
    "# the cache directory for the model\n",
    "CACHE_DIR = mlrun.mlconf.artifact_path\n",
    "CACHE_DIR = (\n",
    "    CACHE_DIR.replace(\"v3io://\", \"/v3io\").replace(\"{{run.project}}\", project.name)\n",
    "    + \"/cache\"\n",
    ")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. vLLM Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Test\n",
    "\n",
    "```vllm-pod.yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: vllm-shell\n",
    "  namespace: mlrun\n",
    "spec:\n",
    "  restartPolicy: Never\n",
    "  runtimeClassName: nvidia\n",
    "  containers:\n",
    "  - name: vllm-container\n",
    "    image: registry-service.mlrun.svc.cluster.local/vllm-batch:0.0.1\n",
    "    command: [\"/bin/bash\"]\n",
    "    stdin: true\n",
    "    tty: true\n",
    "    resources:\n",
    "      limits:\n",
    "        nvidia.com/gpu: \"1\"\n",
    "    imagePullPolicy: Always\n",
    "  imagePullSecrets:\n",
    "  - name: registry-credentials\n",
    "```\n",
    "Now, run the following commands in your terminal:\n",
    "\n",
    "```bash\n",
    "# Apply the YAML file to create the pod\n",
    "sudo kubectl apply -f vllm-pod.yaml\n",
    "\n",
    "# Attach to the running pod to get your interactive shell\n",
    "sudo kubectl attach -n mlrun -it vllm-shell\n",
    "\n",
    "# test in pod\n",
    "nvidia-smi\n",
    "python3 -c 'import torch; print(torch.__version__); print(torch.version.cuda)'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage Remote Registry\n",
    "\n",
    "https://stackoverflow.com/questions/25436742/how-to-delete-images-from-a-private-docker-registry\n",
    "\n",
    "```bash\n",
    "# list repositories\n",
    "curl -s dragon:30500/v2/_catalog -u mlrun:mlpass | jq\n",
    "```\n",
    "\n",
    "Delete a repository:\n",
    "\n",
    "```bash\n",
    "REPO=\"vllm-batch\"\n",
    "REG=\"dragon:30500\"\n",
    "AUTH=\"mlrun:mlpass\"\n",
    "\n",
    "for tag in $(curl -s -u $AUTH http://$REG/v2/$REPO/tags/list | jq -r '.tags[]'); do\n",
    "  digest=$(curl -sI -H \"Accept: application/vnd.docker.distribution.manifest.v2+json\" \\\n",
    "    -u $AUTH http://$REG/v2/$REPO/manifests/$tag | awk -F': ' '/Docker-Content-Digest/ {print $2}' | tr -d $'\\r')\n",
    "  echo \"Deleting $REPO:$tag -> $digest\"\n",
    "  curl -s -X DELETE -u $AUTH http://$REG/v2/$REPO/manifests/$digest\n",
    "done\n",
    "```\n",
    "\n",
    "\n",
    "Garbage collect the registry:\n",
    "\n",
    "```bash\n",
    "docker run --rm \\\n",
    "  -v /home/johnny/swan/opt/registry:/var/lib/registry \\\n",
    "  registry:3 \\\n",
    "  sh -c 'printf \"%s\\n\" \\\n",
    "\"version: 0.1\" \\\n",
    "\"storage:\" \\\n",
    "\"  delete:\" \\\n",
    "\"    enabled: true\" \\\n",
    "\"  filesystem:\" \\\n",
    "\"    rootdirectory: /var/lib/registry\" \\\n",
    "| tee /tmp/c.yml && registry garbage-collect /tmp/c.yml'\n",
    "\n",
    "\n",
    "\n",
    "docker run --rm \\\n",
    "  -v /home/johnny/swan/opt/registry:/var/lib/registry \\\n",
    "  registry:3 \\\n",
    "  sh -c 'printf \"%s\\n\" \\\n",
    "\"version: 0.1\" \\\n",
    "\"storage:\" \\\n",
    "\"  delete:\" \\\n",
    "\"    enabled: true\" \\\n",
    "\"  filesystem:\" \\\n",
    "\"    rootdirectory: /var/lib/registry\" \\\n",
    "| tee /tmp/c.yml && registry garbage-collect -m /tmp/c.yml'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM dragon:30500/mlrun/mlrun:1.9.1\n",
    "\n",
    "RUN pip install --upgrade pip \\\n",
    " && pip install --no-cache-dir --force-reinstall \\\n",
    "      numpy==1.26.4 \\\n",
    "      pandas==2.1.4 \\\n",
    "      mlrun==1.9.1 \\\n",
    "      vllm==0.10.0 \n",
    "      \n",
    "# RUN pip install --no-cache-dir \\\n",
    "#     torch==2.7.1 \\\n",
    "#     torchvision==2.7.1 \\\n",
    "#     torchaudio==0.22.1 \\\n",
    "#     --index-url https://download.pytorch.org/whl/cu124\n",
    "    \n",
    "# RUN pip install --no-cache-dir \\\n",
    "#     vllm==0.10.0 \n",
    "\n",
    "RUN touch /tmp/.vllm_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: docker\n"
     ]
    }
   ],
   "source": [
    "!docker build -t dragon:30500/vllm-batch:0.0.1 -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker run -it --rm --gpus all dragon:30500/vllm-batch:0.0.1 /bin/bash\n",
    "\n",
    "# sudo ctr image pull --plain-http=true --user \"mlrun:mlpass\" dragon:30500/vllm-batch:0.0.1\n",
    "\n",
    "# sudo ctr run --rm --runtime io.containerd.runc.v2 --runc-binary /usr/bin/nvidia-container-runtime \\\n",
    "#  dragon:30500/vllm-batch:0.0.1 test-gpu \\\n",
    "#  bash -c \"nvidia-smi\"\n",
    "\n",
    "\n",
    "# sudo ctr run --rm -t \\\n",
    "#   --runc-binary=/usr/bin/nvidia-container-runtime \\\n",
    "#   --env NVIDIA_VISIBLE_DEVICES=all \\\n",
    "#   dragon:30500/vllm-batch:0.0.1 test-gpu \\\n",
    "#   sh -c \"nvidia-smi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Push the Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: docker\n"
     ]
    }
   ],
   "source": [
    "!docker push dragon:30500/vllm-batch:0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_function():\n",
    "    global project, MODEL_ID, CACHE_DIR\n",
    "\n",
    "    image = \"registry-service.mlrun.svc.cluster.local/vllm-batch:0.0.1\"\n",
    "\n",
    "    llm_func = project.set_function(\n",
    "        func=\"src/06-02_vllm.py\",\n",
    "        name=\"llm-batch\",\n",
    "        kind=\"job\",\n",
    "        image=image,\n",
    "        handler=\"vllm_batch\",\n",
    "        tag=\"v0.0.1\",\n",
    "    )\n",
    "\n",
    "    llm_func.with_limits(gpus=1, gpu_type='nvidia.com/gpu')\n",
    "\n",
    "    llm_func.set_envs({\n",
    "        \"MODEL_ID\": MODEL_ID,\n",
    "        \"CACHE_DIR\": CACHE_DIR,\n",
    "    })\n",
    "    \n",
    "    llm_func.save()\n",
    "\n",
    "    return llm_func\n",
    "\n",
    "## create the function\n",
    "llm_func = create_llm_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-06 13:38:38,789 [warning] It is recommended to use k8s secret (specify secret_name), specifying aws_access_key/aws_secret_key directly is unsafe.\n",
      "> 2025-08-06 13:38:38,915 [info] Started building image: .mlrun/func-llm-batch-johannes-llm-batch:v0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `overwrite_build_params` parameter default will change from 'False' to 'True' in 1.10.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest registry-service.mlrun.svc.cluster.local/vllm-batch:0.0.1 \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image registry-service.mlrun.svc.cluster.local/vllm-batch:0.0.1 from registry registry-service.mlrun.svc.cluster.local \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest registry-service.mlrun.svc.cluster.local/vllm-batch:0.0.1 \n",
      "\u001b[36mINFO\u001b[0m[0000] Returning cached image manifest              \n",
      "\u001b[36mINFO\u001b[0m[0000] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Building stage 'registry-service.mlrun.svc.cluster.local/vllm-batch:0.0.1' [idx: '0', base-idx: '-1'] \n",
      "\u001b[36mINFO\u001b[0m[0000] Skipping unpacking as no commands require it. \n",
      "\u001b[36mINFO\u001b[0m[0000] Pushing image to registry-service.mlrun.svc.cluster.local/mlrun/func-llm-batch-johannes-llm-batch:v0.0.1 \n",
      "\u001b[36mINFO\u001b[0m[0000] Pushed registry-service.mlrun.svc.cluster.local/mlrun/func-llm-batch-johannes-llm-batch@sha256:685f46c42a51299c06dc9fa3bf296bb8d42fbc57df0e25ee07b85c3a45788ac9 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BuildStatus(ready=True, outputs={'image': '.mlrun/func-llm-batch-johannes-llm-batch:v0.0.1'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the function\n",
    "project.build_function(function='llm-batch', force_build=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initial Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-06 13:39:05,405 [info] Storing function: {\"db\":\"http://dragon.local:30070\",\"name\":\"llm-batch-vllm-batch\",\"uid\":\"f6a079a10edd42c1aeeb9403eefba1cf\"}\n",
      "> 2025-08-06 13:39:05,627 [info] Job is running in the background, pod: llm-batch-vllm-batch-h5sj4\n",
      "INFO 08-06 11:39:10 [__init__.py:235] Automatically detected platform cuda.\n",
      "Received event: {}\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3090 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "> 2025-08-06 11:39:11,366 [info] To track results use the CLI: {\"info_cmd\":\"mlrun get run f6a079a10edd42c1aeeb9403eefba1cf -p llm-batch-johannes\",\"logs_cmd\":\"mlrun logs f6a079a10edd42c1aeeb9403eefba1cf -p llm-batch-johannes\"}\n",
      "> 2025-08-06 11:39:11,367 [info] Run execution finished: {\"name\":\"llm-batch-vllm-batch\",\"status\":\"completed\"}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "\n",
       "  // Get the base URL of the current notebook\n",
       "  var baseUrl = window.location.origin;\n",
       "\n",
       "  // Construct the full URL\n",
       "  var fullUrl = new URL(el.title, baseUrl).href;\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = fullUrl\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (fullUrl.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", fullUrl);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = fullUrl;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>state</th>\n",
       "      <th>kind</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>llm-batch-johannes</td>\n",
       "      <td><div title=\"f6a079a10edd42c1aeeb9403eefba1cf\">...fba1cf</div></td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 06 11:39:07</td>\n",
       "      <td>2025-08-06 11:39:11.362794+00:00</td>\n",
       "      <td>completed</td>\n",
       "      <td>run</td>\n",
       "      <td>llm-batch-vllm-batch</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=johannes</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=johannes</div><div class=\"dictlist\">mlrun/client_version=1.9.1</div><div class=\"dictlist\">mlrun/client_python_version=3.9.23</div><div class=\"dictlist\">host=llm-batch-vllm-batch-h5sj4</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">event={}</div></td>\n",
       "      <td><div class=\"dictlist\">return=Device: cuda, Model ID: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B, Cache Directory: s3://mlrun/projects/llm-batch-johannes/artifacts/cache, GPU Available: True</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result34ab700d-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result34ab700d-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result34ab700d\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result34ab700d-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods </b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-06 13:39:18,714 [info] Run execution finished: {\"name\":\"llm-batch-vllm-batch\",\"status\":\"completed\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x1662ebd30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply code changes\n",
    "#llm_func = create_llm_function() \n",
    "\n",
    "# run the function\n",
    "project.run_function(\n",
    "    function='llm-batch',\n",
    "    params={\n",
    "        \"event\": event\n",
    "    },\n",
    "    watch=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
