{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd58bbde",
   "metadata": {},
   "source": [
    "# 06-05: vLLM Offline Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e907a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad002e54",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a4fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"Tiny-LLM\"\n",
    "#MODEL_NAME = \"deepseek-r1-distill-qwen-14b-awq\"\n",
    "MODEL_NAME = \"Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "\n",
    "project_name = \"test-vllm-integration\" # the project name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef83cf9",
   "metadata": {},
   "source": [
    "### 1.1 Load The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee81e501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-22 16:34:43,286 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "Full project name: test-vllm-integration\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name,\n",
    "    user_project=False)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f0280",
   "metadata": {},
   "source": [
    "## 2. Offine Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b5e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prompts to use for inference\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "    \"What are the main differences between Python and Java?\",\n",
    "    \"How does a neural network work?\",\n",
    "    \"What is the significance of the Turing test in AI?\"\n",
    "]\n",
    "\n",
    "# Sampling parameters for the model\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 50 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b23dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-22 16:34:43,316 [warning] It is recommended to use k8s secret (specify secret_name), specifying aws_access_key/aws_secret_key directly is unsafe.\n",
      "> 2025-08-22 16:34:43,321 [info] Storing function: {\"db\":\"http://dragon.local:30070\",\"name\":\"vllm-offile-inference-offline-inference-handler\",\"uid\":\"c1d855a4fe134c60ac7a065c2cec48b3\"}\n",
      "> 2025-08-22 16:34:43,441 [info] Job is running in the background, pod: vllm-offile-inference-offline-inference-handler-m8hf5\n",
      "INFO 08-22 14:34:48 [__init__.py:235] Automatically detected platform cuda.\n",
      "> 2025-08-22 14:34:48,705 [info] Running offline inference for model Mistral-7B-Instruct-v0.2-AWQ with 5 prompts.\n",
      "> 2025-08-22 14:34:48,705 [info] Running offline inference...\n",
      "> 2025-08-22 14:34:48,710 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "> 2025-08-22 14:34:48,713 [info] Downloading tokenizer for model Mistral-7B-Instruct-v0.2-AWQ\n",
      "> 2025-08-22 14:34:48,713 [info] Downloading tokenizer files to temporary directory: /tmp/vllm_tokenizer_wiczdnv9\n",
      "> 2025-08-22 14:34:48,717 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "> 2025-08-22 14:34:48,803 [info] ...Downloading config.json to /tmp/vllm_tokenizer_wiczdnv9\n",
      "> 2025-08-22 14:34:48,806 [info] ...Downloading generation_config.json to /tmp/vllm_tokenizer_wiczdnv9\n",
      "> 2025-08-22 14:34:48,807 [info] ...Downloading special_tokens_map.json to /tmp/vllm_tokenizer_wiczdnv9\n",
      "> 2025-08-22 14:34:48,809 [info] ...Downloading tokenizer.json to /tmp/vllm_tokenizer_wiczdnv9\n",
      "> 2025-08-22 14:34:48,813 [info] ...Downloading tokenizer.model to /tmp/vllm_tokenizer_wiczdnv9\n",
      "> 2025-08-22 14:34:48,815 [info] ...Downloading tokenizer_config.json to /tmp/vllm_tokenizer_wiczdnv9\n",
      "INFO 08-22 14:34:53 [config.py:1604] Using max model len 32768\n",
      "INFO 08-22 14:34:53 [awq_marlin.py:116] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-22 14:34:53 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-22 14:34:54 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-22 14:34:54 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/tmp/tmpro0szsn_', speculative_config=None, tokenizer='/tmp/vllm_tokenizer_wiczdnv9', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.RUNAI_STREAMER, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-22 14:34:55 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-22 14:34:55 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.\n",
      "INFO 08-22 14:34:55 [gpu_model_runner.py:1843] Starting to load model /tmp/tmpro0szsn_...\n",
      "INFO 08-22 14:34:55 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-22 14:34:55 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "[RunAI Streamer] CPU Buffer size: 8 Bytes for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n",
      "[RunAI Streamer] CPU Buffer size: 82.0 KiB for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n",
      "[RunAI Streamer] CPU Buffer size: 3.9 GiB for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n",
      "Loading safetensors using Runai Model Streamer:   0% Completed | 0/739 [00:00<?, ?it/s]\n",
      "Loading safetensors using Runai Model Streamer:   5% Completed | 35/739 [00:00<00:02, 341.07it/s]\n",
      "Loading safetensors using Runai Model Streamer:   9% Completed | 70/739 [00:00<00:02, 235.58it/s]\n",
      "Loading safetensors using Runai Model Streamer:  13% Completed | 96/739 [00:00<00:02, 229.31it/s]\n",
      "Loading safetensors using Runai Model Streamer:  18% Completed | 134/739 [00:16<01:52,  5.38it/s]\n",
      "Loading safetensors using Runai Model Streamer:  24% Completed | 177/739 [00:16<01:01,  9.17it/s]\n",
      "Loading safetensors using Runai Model Streamer:  28% Completed | 209/739 [00:16<00:40, 13.02it/s]\n",
      "Loading safetensors using Runai Model Streamer:  32% Completed | 238/739 [00:16<00:28, 17.70it/s]\n",
      "Loading safetensors using Runai Model Streamer:  36% Completed | 263/739 [00:17<00:23, 20.65it/s]\n",
      "Loading safetensors using Runai Model Streamer:  42% Completed | 314/739 [00:17<00:12, 35.18it/s]\n",
      "Loading safetensors using Runai Model Streamer:  46% Completed | 342/739 [00:17<00:08, 44.96it/s]\n",
      "Loading safetensors using Runai Model Streamer:  50% Completed | 370/739 [00:17<00:06, 57.00it/s]\n",
      "Loading safetensors using Runai Model Streamer:  54% Completed | 396/739 [00:18<00:06, 52.00it/s]\n",
      "Loading safetensors using Runai Model Streamer:  59% Completed | 435/739 [00:18<00:04, 74.92it/s]\n",
      "Loading safetensors using Runai Model Streamer:  62% Completed | 460/739 [00:18<00:03, 87.42it/s]\n",
      "Loading safetensors using Runai Model Streamer:  71% Completed | 526/739 [00:18<00:01, 150.28it/s]\n",
      "Read throughput is 224.76 MB per second \n",
      "Loading safetensors using Runai Model Streamer:  93% Completed | 687/739 [00:18<00:00, 348.14it/s]\n",
      "Loading safetensors using Runai Model Streamer: 100% Completed | 739/739 [00:18<00:00, 39.87it/s]\n",
      " \n",
      "[RunAI Streamer] Overall time to stream 3.9 GiB of all files: 34.61s, 114.4 MiB/s\n",
      "INFO 08-22 14:35:30 [gpu_model_runner.py:1892] Model loading took 3.8813 GiB and 35.119071 seconds\n",
      "INFO 08-22 14:35:36 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/ec5f047f76/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-22 14:35:36 [backends.py:541] Dynamo bytecode transform time: 5.64 s\n",
      "INFO 08-22 14:35:38 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 08-22 14:35:57 [backends.py:215] Compiling a graph for dynamic shape takes 19.91 s\n",
      "INFO 08-22 14:36:03 [monitor.py:34] torch.compile takes 25.56 s in total\n",
      "INFO 08-22 14:36:04 [gpu_worker.py:255] Available KV cache memory: 16.49 GiB\n",
      "INFO 08-22 14:36:05 [kv_cache_utils.py:833] GPU KV cache size: 135,088 tokens\n",
      "INFO 08-22 14:36:05 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 4.12x\n",
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:04<00:00, 15.89it/s]\n",
      "INFO 08-22 14:36:09 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 0.54 GiB\n",
      "INFO 08-22 14:36:09 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.80 seconds\n",
      "Adding requests: 100%|██████████| 5/5 [00:00<00:00, 1793.66it/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:00<00:00, 13.12it/s, est. speed input: 136.52 toks/s, output: 656.34 toks/s]\n",
      "> 2025-08-22 14:36:10,175 [info] Offline inference completed with 5 responses.\n",
      "> 2025-08-22 14:36:10,378 [info] To track results use the CLI: {\"info_cmd\":\"mlrun get run c1d855a4fe134c60ac7a065c2cec48b3 -p test-vllm-integration\",\"logs_cmd\":\"mlrun logs c1d855a4fe134c60ac7a065c2cec48b3 -p test-vllm-integration\"}\n",
      "> 2025-08-22 14:36:10,379 [info] Run execution finished: {\"name\":\"vllm-offile-inference-offline-inference-handler\",\"status\":\"completed\"}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "\n",
       "  // Get the base URL of the current notebook\n",
       "  var baseUrl = window.location.origin;\n",
       "\n",
       "  // Construct the full URL\n",
       "  var fullUrl = new URL(el.title, baseUrl).href;\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = fullUrl\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (fullUrl.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", fullUrl);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = fullUrl;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>state</th>\n",
       "      <th>kind</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>test-vllm-integration</td>\n",
       "      <td><div title=\"c1d855a4fe134c60ac7a065c2cec48b3\">...ec48b3</div></td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 22 14:34:45</td>\n",
       "      <td>2025-08-22 14:36:10.371946+00:00</td>\n",
       "      <td>completed</td>\n",
       "      <td>run</td>\n",
       "      <td>vllm-offile-inference-offline-inference-handler</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=johannes</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=johannes</div><div class=\"dictlist\">mlrun/client_version=1.9.1</div><div class=\"dictlist\">mlrun/client_python_version=3.10.18</div><div class=\"dictlist\">host=vllm-offile-inference-offline-inference-handler-m8hf5</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">model_name=Mistral-7B-Instruct-v0.2-AWQ</div><div class=\"dictlist\">prompts=['What is the capital of France?', 'Explain the theory of relativity in simple terms.', 'What are the main differences between Python and Java?', 'How does a neural network work?', 'What is the significance of the Turing test in AI?']</div><div class=\"dictlist\">sampling_params={'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 50}</div><div class=\"dictlist\">max_model_len=32768</div></td>\n",
       "      <td><div class=\"dictlist\">outputs=[{'prompt': 'What is the capital of France?', 'response': '\\n\\nThe capital city of France is Paris. It is the most populous city in France, and it is also one of the biggest cities in Europe. Paris is known for its rich history, beautiful architecture, museums, and art scene.'}, {'prompt': 'Explain the theory of relativity in simple terms.', 'response': '\\n\\nThe theory of relativity is a set of scientific ideas developed by Albert Einstein in the early 1900s. It changed the way we understand space and time.\\n\\nThere are two main parts to the theory of relativity'}, {'prompt': 'What are the main differences between Python and Java?', 'response': ' Python and Java are two of the most popular programming languages today, each with its own strengths and weaknesses. While they share some similarities, they have significant differences that make them better suited for different use cases. Here are some of the main differences'}, {'prompt': 'How does a neural network work?', 'response': '\\n\\nA neural network is a type of machine learning model that is inspired by the human brain. It consists of interconnected nodes, called neurons, which process information using a series of non-linear transformations. These transformations allow the network'}, {'prompt': 'What is the significance of the Turing test in AI?', 'response': \"\\n\\nThe Turing test is a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. It was proposed by Alan Turing in 1950 as a\"}]</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resultcc1aba2b-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resultcc1aba2b-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resultcc1aba2b\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resultcc1aba2b-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods </b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-22 16:36:14,670 [info] Run execution finished: {\"name\":\"vllm-offile-inference-offline-inference-handler\",\"status\":\"completed\"}\n"
     ]
    }
   ],
   "source": [
    "# create the function\n",
    "fn_inference = project.set_function(\n",
    "    name=\"vllm-offile-inference\",\n",
    "    func=\"../../src/functions/vllm_model_server.py\",\n",
    "    kind=\"job\",\n",
    "    handler=\"offline_inference_handler\",\n",
    "    image=\"registry-service.mlrun.svc.cluster.local/foulds/mlrun-vllm:0.10.0\"\n",
    ")\n",
    "\n",
    "fn_inference.with_limits(gpus=1)\n",
    "\n",
    "# create the task to run the function\n",
    "task = mlrun.new_task(\n",
    "    name=\"vllm-offline-inference-task\",\n",
    "    project=project_name,\n",
    "\n",
    ")    \n",
    "\n",
    "# run the function\n",
    "run_output = fn_inference.run(\n",
    "    task=task,\n",
    "    params={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"prompts\": prompts,\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"max_model_len\" : 32768,\n",
    "    },\n",
    "    local=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2bb46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: c1d855a4fe134c60ac7a065c2cec48b3\n",
      "run_output state: completed\n",
      "run_output results: {'outputs': [{'prompt': 'What is the capital of France?', 'response': '\\n\\nThe capital city of France is Paris. It is the most populous city in France, and it is also one of the biggest cities in Europe. Paris is known for its rich history, beautiful architecture, museums, and art scene.'}, {'prompt': 'Explain the theory of relativity in simple terms.', 'response': '\\n\\nThe theory of relativity is a set of scientific ideas developed by Albert Einstein in the early 1900s. It changed the way we understand space and time.\\n\\nThere are two main parts to the theory of relativity'}, {'prompt': 'What are the main differences between Python and Java?', 'response': ' Python and Java are two of the most popular programming languages today, each with its own strengths and weaknesses. While they share some similarities, they have significant differences that make them better suited for different use cases. Here are some of the main differences'}, {'prompt': 'How does a neural network work?', 'response': '\\n\\nA neural network is a type of machine learning model that is inspired by the human brain. It consists of interconnected nodes, called neurons, which process information using a series of non-linear transformations. These transformations allow the network'}, {'prompt': 'What is the significance of the Turing test in AI?', 'response': \"\\n\\nThe Turing test is a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. It was proposed by Alan Turing in 1950 as a\"}]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Run ID: {run_output.metadata.uid}\")\n",
    "print(f\"run_output state: {run_output.status.state}\")\n",
    "print(f\"run_output results: {run_output.status.results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb13522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### What is the capital of France?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The capital city of France is Paris. It is the most populous city in France, and it is also one of the biggest cities in Europe. Paris is known for its rich history, beautiful architecture, museums, and art scene."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Explain the theory of relativity in simple terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The theory of relativity is a set of scientific ideas developed by Albert Einstein in the early 1900s. It changed the way we understand space and time.\n",
       "\n",
       "There are two main parts to the theory of relativity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### What are the main differences between Python and Java?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Python and Java are two of the most popular programming languages today, each with its own strengths and weaknesses. While they share some similarities, they have significant differences that make them better suited for different use cases. Here are some of the main differences"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### How does a neural network work?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "A neural network is a type of machine learning model that is inspired by the human brain. It consists of interconnected nodes, called neurons, which process information using a series of non-linear transformations. These transformations allow the network"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### What is the significance of the Turing test in AI?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The Turing test is a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. It was proposed by Alan Turing in 1950 as a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for output in run_output.status.results['outputs']:\n",
    "    display(Markdown(f\"### {output['prompt']}\"))\n",
    "    display(Markdown(output['response']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
