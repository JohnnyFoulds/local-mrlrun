{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd58bbde",
   "metadata": {},
   "source": [
    "# 06-05: vLLM Offline Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e907a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad002e54",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a4fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"Tiny-LLM\"\n",
    "MODEL_NAME = \"Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "project_name = \"test-vllm-integration\" # the project name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef83cf9",
   "metadata": {},
   "source": [
    "### 1.1 Load The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee81e501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-12 16:53:21,829 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "Full project name: test-vllm-integration\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=project_name,\n",
    "    user_project=False)\n",
    "\n",
    "# Display the current project name\n",
    "project_name = project.metadata.name\n",
    "print(f'Full project name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f0280",
   "metadata": {},
   "source": [
    "## 2. Offine Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b5e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prompts to use for inference\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "    \"What are the main differences between Python and Java?\",\n",
    "    \"How does a neural network work?\",\n",
    "    \"What is the significance of the Turing test in AI?\"\n",
    "]\n",
    "\n",
    "# Sampling parameters for the model\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 25 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b23dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-12 16:53:22,023 [warning] It is recommended to use k8s secret (specify secret_name), specifying aws_access_key/aws_secret_key directly is unsafe.\n",
      "> 2025-08-12 16:53:22,032 [info] Storing function: {\"db\":\"http://dragon.local:30070\",\"name\":\"vllm-offile-inference-offline-inference-handler\",\"uid\":\"c1fc392995274cd089c3c6c565d9afe5\"}\n",
      "> 2025-08-12 16:53:22,137 [info] Job is running in the background, pod: vllm-offile-inference-offline-inference-handler-trpg8\n",
      "INFO 08-12 14:53:26 [__init__.py:235] Automatically detected platform cuda.\n",
      "> 2025-08-12 14:53:27,443 [info] Running offline inference for model Mistral-7B-Instruct-v0.2-AWQ with 5 prompts.\n",
      "> 2025-08-12 14:53:27,443 [info] Running offline inference...\n",
      "> 2025-08-12 14:53:27,448 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "> 2025-08-12 14:53:27,451 [info] Downloading tokenizer for model Mistral-7B-Instruct-v0.2-AWQ\n",
      "> 2025-08-12 14:53:27,451 [info] Downloading tokenizer files to temporary directory: /tmp/vllm_tokenizer_cq5o8w6g\n",
      "> 2025-08-12 14:53:27,455 [info] Project loaded successfully: {\"project_name\":\"test-vllm-integration\"}\n",
      "> 2025-08-12 14:53:27,538 [info] ...Downloading config.json to /tmp/vllm_tokenizer_cq5o8w6g\n",
      "> 2025-08-12 14:53:27,540 [info] ...Downloading generation_config.json to /tmp/vllm_tokenizer_cq5o8w6g\n",
      "> 2025-08-12 14:53:27,542 [info] ...Downloading special_tokens_map.json to /tmp/vllm_tokenizer_cq5o8w6g\n",
      "> 2025-08-12 14:53:27,544 [info] ...Downloading tokenizer.json to /tmp/vllm_tokenizer_cq5o8w6g\n",
      "> 2025-08-12 14:53:27,548 [info] ...Downloading tokenizer.model to /tmp/vllm_tokenizer_cq5o8w6g\n",
      "> 2025-08-12 14:53:27,550 [info] ...Downloading tokenizer_config.json to /tmp/vllm_tokenizer_cq5o8w6g\n",
      "INFO 08-12 14:53:32 [config.py:1604] Using max model len 32768\n",
      "INFO 08-12 14:53:32 [awq_marlin.py:116] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-12 14:53:32 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-12 14:53:32 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-12 14:53:32 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/tmp/tmporhau6_0', speculative_config=None, tokenizer='/tmp/vllm_tokenizer_cq5o8w6g', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.RUNAI_STREAMER, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-12 14:53:33 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-12 14:53:33 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.\n",
      "INFO 08-12 14:53:33 [gpu_model_runner.py:1843] Starting to load model /tmp/tmporhau6_0...\n",
      "INFO 08-12 14:53:34 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-12 14:53:34 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "[RunAI Streamer] CPU Buffer size: 8 Bytes for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n",
      "[RunAI Streamer] CPU Buffer size: 82.0 KiB for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n",
      "[RunAI Streamer] CPU Buffer size: 3.9 GiB for files: ['s3://mlrun/projects/test-vllm-integration/artifacts/Mistral-7B-Instruct-v0.2-AWQ/model.safetensors']\n",
      "Loading safetensors using Runai Model Streamer:   0% Completed | 0/739 [00:00<?, ?it/s]\n",
      "Loading safetensors using Runai Model Streamer:   4% Completed | 33/739 [00:00<00:02, 320.33it/s]\n",
      "Loading safetensors using Runai Model Streamer:   9% Completed | 66/739 [00:00<00:02, 278.71it/s]\n",
      "Loading safetensors using Runai Model Streamer:  13% Completed | 95/739 [00:00<00:02, 215.98it/s]\n",
      "Loading safetensors using Runai Model Streamer:  18% Completed | 134/739 [00:15<01:43,  5.84it/s]\n",
      "Loading safetensors using Runai Model Streamer:  24% Completed | 174/739 [00:15<00:58,  9.59it/s]\n",
      "Loading safetensors using Runai Model Streamer:  27% Completed | 203/739 [00:16<00:44, 12.06it/s]\n",
      "Loading safetensors using Runai Model Streamer:  33% Completed | 244/739 [00:16<00:26, 18.83it/s]\n",
      "Loading safetensors using Runai Model Streamer:  37% Completed | 271/739 [00:17<00:22, 20.82it/s]\n",
      "Loading safetensors using Runai Model Streamer:  41% Completed | 306/739 [00:17<00:14, 29.93it/s]\n",
      "Loading safetensors using Runai Model Streamer:  46% Completed | 338/739 [00:17<00:09, 40.81it/s]\n",
      "Loading safetensors using Runai Model Streamer:  49% Completed | 364/739 [00:17<00:07, 51.90it/s]\n",
      "Loading safetensors using Runai Model Streamer:  53% Completed | 390/739 [00:17<00:05, 65.55it/s]\n",
      "Loading safetensors using Runai Model Streamer:  56% Completed | 415/739 [00:18<00:05, 54.75it/s]\n",
      "Loading safetensors using Runai Model Streamer:  59% Completed | 434/739 [00:18<00:04, 64.79it/s]\n",
      "Loading safetensors using Runai Model Streamer:  73% Completed | 540/739 [00:18<00:01, 163.41it/s]\n",
      "Loading safetensors using Runai Model Streamer:  99% Completed | 729/739 [00:18<00:00, 377.28it/s]\n",
      "Read throughput is 224.87 MB per second \n",
      "Loading safetensors using Runai Model Streamer: 100% Completed | 739/739 [00:18<00:00, 39.92it/s]\n",
      " \n",
      "[RunAI Streamer] Overall time to stream 3.9 GiB of all files: 34.59s, 114.5 MiB/s\n",
      "INFO 08-12 14:54:09 [gpu_model_runner.py:1892] Model loading took 3.8813 GiB and 35.079140 seconds\n",
      "INFO 08-12 14:54:15 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/979abe24a3/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-12 14:54:15 [backends.py:541] Dynamo bytecode transform time: 5.59 s\n",
      "INFO 08-12 14:54:17 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 08-12 14:54:35 [backends.py:215] Compiling a graph for dynamic shape takes 19.78 s\n",
      "INFO 08-12 14:54:42 [monitor.py:34] torch.compile takes 25.36 s in total\n",
      "INFO 08-12 14:54:43 [gpu_worker.py:255] Available KV cache memory: 16.49 GiB\n",
      "INFO 08-12 14:54:43 [kv_cache_utils.py:833] GPU KV cache size: 135,088 tokens\n",
      "INFO 08-12 14:54:43 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 4.12x\n",
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:04<00:00, 15.63it/s]\n",
      "INFO 08-12 14:54:48 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 0.54 GiB\n",
      "INFO 08-12 14:54:48 [core.py:193] init engine (profile, create kv cache, warmup model) took 38.78 seconds\n",
      "Adding requests: 100%|██████████| 5/5 [00:00<00:00, 2505.26it/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:00<00:00, 24.62it/s, est. speed input: 256.25 toks/s, output: 615.96 toks/s]\n",
      "> 2025-08-12 14:54:48,747 [info] Offline inference completed with 5 responses.\n",
      "> 2025-08-12 14:54:48,996 [info] To track results use the CLI: {\"info_cmd\":\"mlrun get run c1fc392995274cd089c3c6c565d9afe5 -p test-vllm-integration\",\"logs_cmd\":\"mlrun logs c1fc392995274cd089c3c6c565d9afe5 -p test-vllm-integration\"}\n",
      "> 2025-08-12 14:54:48,997 [info] Run execution finished: {\"name\":\"vllm-offile-inference-offline-inference-handler\",\"status\":\"completed\"}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "\n",
       "  // Get the base URL of the current notebook\n",
       "  var baseUrl = window.location.origin;\n",
       "\n",
       "  // Construct the full URL\n",
       "  var fullUrl = new URL(el.title, baseUrl).href;\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = fullUrl\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (fullUrl.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", fullUrl);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = fullUrl;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>state</th>\n",
       "      <th>kind</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>test-vllm-integration</td>\n",
       "      <td><div title=\"c1fc392995274cd089c3c6c565d9afe5\">...d9afe5</div></td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 12 14:53:23</td>\n",
       "      <td>2025-08-12 14:54:48.988728+00:00</td>\n",
       "      <td>completed</td>\n",
       "      <td>run</td>\n",
       "      <td>vllm-offile-inference-offline-inference-handler</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=johannes</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=johannes</div><div class=\"dictlist\">mlrun/client_version=1.9.1</div><div class=\"dictlist\">mlrun/client_python_version=3.10.18</div><div class=\"dictlist\">host=vllm-offile-inference-offline-inference-handler-trpg8</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">model_name=Mistral-7B-Instruct-v0.2-AWQ</div><div class=\"dictlist\">prompts=['What is the capital of France?', 'Explain the theory of relativity in simple terms.', 'What are the main differences between Python and Java?', 'How does a neural network work?', 'What is the significance of the Turing test in AI?']</div><div class=\"dictlist\">sampling_params={'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 25}</div></td>\n",
       "      <td><div class=\"dictlist\">outputs=[{'prompt': 'What is the capital of France?', 'response': '\\n\\nThe capital city of France is Paris. It is the most populous city in France, and it is also one'}, {'prompt': 'Explain the theory of relativity in simple terms.', 'response': '\\n\\nThe theory of relativity is a set of scientific ideas developed by Albert Einstein in the early 1900'}, {'prompt': 'What are the main differences between Python and Java?', 'response': ' Python and Java are two of the most popular programming languages today, each with its own strengths and weaknesses. While they share'}, {'prompt': 'How does a neural network work?', 'response': '\\n\\nA neural network is a type of machine learning model that is inspired by the human brain. It consists of interconnected'}, {'prompt': 'What is the significance of the Turing test in AI?', 'response': \"\\n\\nThe Turing test is a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indist\"}]</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result64db17b4-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result64db17b4-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result64db17b4\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result64db17b4-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods </b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-08-12 16:54:53,379 [info] Run execution finished: {\"name\":\"vllm-offile-inference-offline-inference-handler\",\"status\":\"completed\"}\n"
     ]
    }
   ],
   "source": [
    "# create the function\n",
    "fn_inference = project.set_function(\n",
    "    name=\"vllm-offile-inference\",\n",
    "    func=\"../../src/functions/vllm_model_server.py\",\n",
    "    kind=\"job\",\n",
    "    handler=\"offline_inference_handler\",\n",
    "    image=\"registry-service.mlrun.svc.cluster.local/foulds/mlrun-vllm:0.10.0\"\n",
    ")\n",
    "\n",
    "fn_inference.with_limits(gpus=1)\n",
    "\n",
    "# create the task to run the function\n",
    "task = mlrun.new_task(\n",
    "    name=\"vllm-offline-inference-task\",\n",
    "    project=project_name,\n",
    "\n",
    ")    \n",
    "\n",
    "# run the function\n",
    "run_output = fn_inference.run(\n",
    "    task=task,\n",
    "    params={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"prompts\": prompts,\n",
    "        \"sampling_params\": sampling_params,\n",
    "    },\n",
    "    local=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2bb46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: c1fc392995274cd089c3c6c565d9afe5\n",
      "run_output state: completed\n",
      "run_output results: {'outputs': [{'prompt': 'What is the capital of France?', 'response': '\\n\\nThe capital city of France is Paris. It is the most populous city in France, and it is also one'}, {'prompt': 'Explain the theory of relativity in simple terms.', 'response': '\\n\\nThe theory of relativity is a set of scientific ideas developed by Albert Einstein in the early 1900'}, {'prompt': 'What are the main differences between Python and Java?', 'response': ' Python and Java are two of the most popular programming languages today, each with its own strengths and weaknesses. While they share'}, {'prompt': 'How does a neural network work?', 'response': '\\n\\nA neural network is a type of machine learning model that is inspired by the human brain. It consists of interconnected'}, {'prompt': 'What is the significance of the Turing test in AI?', 'response': \"\\n\\nThe Turing test is a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indist\"}]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Run ID: {run_output.metadata.uid}\")\n",
    "print(f\"run_output state: {run_output.status.state}\")\n",
    "print(f\"run_output results: {run_output.status.results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-mlrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
